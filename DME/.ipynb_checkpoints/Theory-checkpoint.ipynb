{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\newcommand{BSigma}{{\\mathbf{\\Sigma}}}$\n",
       "$\\newcommand{Bmu}   {{\\mathbf{mu}}}$\n",
       "$\\newcommand{khat} {{\\hat{k}}}$\n",
       "$\\BSigma$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\newcommand{BSigma}{{\\mathbf{\\Sigma}}}$\n",
    "$\\newcommand{Bmu}   {{\\mathbf{mu}}}$\n",
    "$\\newcommand{khat} {{\\hat{k}}}$\n",
    "$\\BSigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "\n",
    "Suppose we know that each frame observation $X$ is generated from a model $\\mathbf{\\Theta} = (\\mathbf{\\rho}, \\mathbf{\\mu}, \\mathbf{\\Sigma})$, a la\n",
    "\n",
    "$$\n",
    "K\\sim \\mathrm{Categorical}(\\mathbf{\\rho}); \\\\\n",
    "X \\mid K \\sim \\mathrm{MVN}(\\mathbf{\\mu}_K, \\mathbf{\\Sigma}_K)\n",
    "$$\n",
    "\n",
    "Suppose we are interested in estimating the underlying cluster $k$ that generated some observations $\\{X_i\\}$.  For example, we could have observed a single-frame pulse, or a pulse of length $\\ell$ frames, and we are tasked with classifying the originating cluster.  This is a simplification of the basecaller's task.\n",
    "\n",
    "In general, the classifier can be arbitrary in nature, but let's assume that we are using a Bayes classifier using the same structure as the generative model.\n",
    "\n",
    "Further, for the moment, let's assume that the generative model and the classification model are the same---this allows us to talk about the \"irreducible error\", or the *Bayes error rate*, the error intrinsic to the generating model.\n",
    "\n",
    "## Notational conventions\n",
    "\n",
    "We let $M$ denote the number of cameras---and thus the dimentionality of $X$.\n",
    "\n",
    "We use the notation $f_k(x)$ to represent the multivariate normal density for cluster $k$, i.e.\n",
    "\n",
    "$$\n",
    "f_k(x) = (2\\pi)^{-{M \\over 2}} |\\Sigma}|^{-{1 \\over 2}}\n",
    "\\exp\\big[ -{1 \\over 2} (x - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1} (x-\\mathbf{\\mu}) \\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "### Single-frame\n",
    "\n",
    "Given a single-frame observation $X$, our classifier outputs\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{k} &= \\arg\\max_{k} \\Pr(X \\mid K=\\hat{k}) \\\\\n",
    "        &= \\arg\\max_{k} \\rho_k f_k(X)\n",
    "\\end{align}         \n",
    "$$\n",
    "\n",
    "The (joint) classification error of type $k \\to \\hat{k}$ in this scheme is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\epsilon_{k \\to \\hat{k}} &= \\Pr(\\hat{K} = \\hat{k}, K = k) \\\\\n",
    "&= \\Pr(K=k) \\int \\Pr(X=x \\mid k) \\Pr(\\hat{K} = \\hat{k} \\mid X=x) \\, dx\n",
    "\\end{align}\n",
    "\n",
    "&= \\rho_k \\int f_k(x) \\frac{\\rho_{\\hat{k}\n",
    "$$\n",
    "\n",
    "The total misclassification error is $\\epsilon = \\sum_{k \\neq \\hat{k}} \\epsilon_{k\\to\\hat{k}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\khat$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
